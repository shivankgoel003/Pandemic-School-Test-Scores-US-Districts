---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - First author
  - Another author
thanks: "Code and data are available at: LINK."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse) # a collection of data-related packages
library(knitr) # for kable
library(kableExtra)
library(here)
library(ggbeeswarm)


#reading cleaned data
rhode = read_csv(
  file = here("outputs/data/cleaned_learningmodel/rhode.csv"),       
  show_col_types = FALSE
)

colorado = read_csv(
  file = here("outputs/data/cleaned_learningmodel/colorado.csv"),       
  show_col_types = FALSE
)

connecticut = read_csv(
  file = here("outputs/data/cleaned_learningmodel/connecticut.csv"),       
  show_col_types = FALSE
)

massachusets = read_csv(
  file = here("outputs/data/cleaned_learningmodel/massachusets.csv"),       
  show_col_types = FALSE
)
minnesota = read_csv(
  file = here("outputs/data/cleaned_learningmodel/minnesota.csv"),       
  show_col_types = FALSE
)

mississippi = read_csv(
  file = here("outputs/data/cleaned_learningmodel/mississippi.csv"),       
  show_col_types = FALSE
)

ohio = read_csv(
  file = here("outputs/data/cleaned_learningmodel/ohio.csv"),       
  show_col_types = FALSE
)
virginia = read_csv(
  file = here("outputs/data/cleaned_learningmodel/virginia.csv"),       
  show_col_types = FALSE
)
westvirginia = read_csv(
  file = here("outputs/data/cleaned_learningmodel/westvirginia.csv"),       
  show_col_types = FALSE
)
winscosin = read_csv(
  file = here("outputs/data/cleaned_learningmodel/winscosin.csv"),       
  show_col_types = FALSE
)

wyoming = read_csv(
  file = here("outputs/data/cleaned_learningmodel/wyoming.csv"),       
  show_col_types = FALSE
)

scores = read_csv(
  file = here("outputs/data/all_scoredata.csv"),       
  show_col_types = FALSE
)
scores_2019 = read_csv(
  file = here("outputs/data/score_ohio.csv"),       
  show_col_types = FALSE
)

```

```{r}
#| label: Vanshika-graphs
#| fig-cap: lets see
#| echo: false
# Plotting passing rates for each state
ggplot(scores, aes(x = state, y = pass, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
# graph of all the passing rate all the different states 


# Calculate the difference between passing rates
pass_diff <- scores$pass - mean(scores$pass)

# Plotting
ggplot(scores, aes(x = state, y = pass_diff, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "Difference in Passing Rates Between States",
       x = "State",
       y = "Difference in Passing Rates") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# The lowest passing rates were Rhode Island

# graph for rode island and its unemployemnt during covid
# Plotting passing rates for each state
rhode_island_data <- subset(scores, state == "RI")
ggplot(scores, aes(x = state , y = unemployment, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "unemployment Across Different States",
       x = "State",
       y = "Unemployemtn Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
# graph of all the passing rate all the different states 

ggplot(scores, aes(x = state , y = cts_pass_advanced, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "advanced Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



ggplot(scores, aes(x = state , y = cts_pass_proficient, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "profient Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(scores, aes(x = state , y = cts_pass_approaching, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "approaching Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(scores, aes(x = state , y = cts_pass_partially, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "Partially Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(scores, aes(x = state , y = cts_pass_below, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "below Passing Rates Across Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# DATA WITH ONLY 2019
ggplot(scores_2019, aes(x = state, y = pass, fill = state)) +
  geom_bar(stat = "identity") +
  labs(title = "Passing Rates In 2019",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#| label: Vanshika-RI
#| fig-cap: lets see
#| echo: false
library(reshape2)
# Select relevant columns for correlation analysis
correlation_data <- scores[, c("pass", "participation", "share_black", "share_white", "share_hisp", "share_lunch", "unemployment")]

# Calculate correlation matrix
correlation_matrix <- cor(correlation_data, use="complete.obs")

# Convert the correlation matrix to a data frame
correlation_df <- melt(correlation_matrix)

# Plotting the correlation heatmap
correlation_heatmap <- ggplot(correlation_df, aes(Var1, Var2, fill=value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low="blue", mid="white", high="red", midpoint=0, limit=c(-1,1), space="Lab", name="Correlation") +
  theme_minimal() +
  labs(title = "Correlation Analysis: Heatmap of Correlation Coefficients",
       x = "Variables",
       y = "Variables") +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Display the heatmap
correlation_heatmap



# Select relevant columns for correlation analysis
correlation_data <- scores[, c("share_inperson", "share_virtual", "share_hybrid", "pass", "participation", "share_black", "share_white", "share_hisp", "unemployment")]

# Calculate correlation matrix
correlation_matrix <- cor(correlation_data, use="complete.obs")

# Convert the correlation matrix to a data frame
correlation_df <- melt(correlation_matrix)

# Plotting the correlation heatmap with vertical x-axis labels
correlation_heatmap <- ggplot(correlation_df, aes(Var1, Var2, fill=value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low="blue", mid="white", high="red", midpoint=0, limit=c(-1,1), space="Lab", name="Correlation") +
  theme_minimal() +
  labs(title = "Correlation Analysis: Heatmap of Correlation Coefficients",
       x = "Variables",
       y = "Variables") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Display the heatmap
correlation_heatmap



# Select relevant columns for correlation analysis
correlation_data <- scores %>%
  select(state, participation, share_inperson)

# Calculate correlation matrix by grouping data by state and then calculating correlations
correlation_matrix <- correlation_data %>%
  group_by(state) %>%
  summarise(correlation = cor(participation, share_inperson))

# Plotting the correlation graph
correlation_graph <- ggplot(correlation_matrix, aes(x = state, y = correlation)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  labs(title = "Correlation between Participation Rate and Share In Person by State",
       x = "State",
       y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the correlation graph
correlation_graph


# coorelation between virtual and participation
# Select relevant columns for correlation analysis
correlation_data <- scores %>%
  select(state, participation, share_virtual)

# Calculate correlation matrix by grouping data by state and then calculating correlations
correlation_matrix <- correlation_data %>%
  group_by(state) %>%
  summarise(correlation = cor(participation, share_virtual))

# Plotting the correlation graph
correlation_graph <- ggplot(correlation_matrix, aes(x = state, y = correlation)) +
  geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
  labs(title = "Correlation between Participation Rate and Share virtual by State",
       x = "State",
       y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the correlation graph
correlation_graph


# corelation between pass and unemployemnt 
# Select relevant columns for correlation analysis
correlation_data <- scores %>%
  select(state, unemployment, pass)

# Calculate correlation matrix by grouping data by state and then calculating correlations
correlation_matrix <- correlation_data %>%
  group_by(state) %>%
  summarise(correlation = cor(unemployment, pass))

# Plotting the correlation graph
correlation_graph <- ggplot(correlation_matrix, aes(x = state, y = correlation)) +
  geom_bar(stat = "identity", fill = "purple", width = 0.5) +
  labs(title = "Correlation between Unemployement Rate and passing rate by State",
       x = "State",
       y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the correlation graph
correlation_graph
```

```{r}
#| label: Random Graphs
#| fig-cap: lets see
#| echo: false

library(ggplot2)

# Passing rate trends over different years and subjects
passing_rate_trends <- ggplot(scores, aes(x = year, y = pass, color = subject)) +
  geom_line() +
  labs(title = "Passing Rate Trends Over Different Years and Subjects",
       x = "Year",
       y = "Passing Rate",
       color = "Subject") +
  theme_minimal()

# Variations between different districts or states
passing_rate_variations <- ggplot(scores, aes(x = state, y = pass, fill = state)) +
  geom_boxplot() +
  labs(title = "Variations in Passing Rates Between Different States",
       x = "State",
       y = "Passing Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the graphs
passing_rate_trends
passing_rate_variations




# Investigating participation rates across districts or states
participation_rates <- ggplot(scores, aes(x = state, y = participation, fill = state)) +
  geom_boxplot() +
  labs(title = "Participation Rates Across Different States",
       x = "State",
       y = "Participation Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the graph
participation_rates



# Relationship between demographic factors and passing rates
demographic_analysis <- ggplot(scores, aes(x = share_black, y = pass)) +
  geom_point() +
  labs(title = "Relationship Between Percentage of Black Students and Passing Rate",
       x = "Percentage of Black Students",
       y = "Passing Rate") +
  theme_minimal()

# Display the graph
demographic_analysis





# Comparing passing rates between different subjects
subject_comparison <- ggplot(scores, aes(x = subject, y = pass, fill = subject)) +
  geom_boxplot() +
  labs(title = "Passing Rates Comparison Between Different Subjects",
       x = "Subject",
       y = "Passing Rate") +
  theme_minimal()

# Display the graph
subject_comparison

```
# Introduction

The COVID-19 pandemic has profoundly disrupted education systems worldwide, compelling schools and districts across the United States to rapidly adapt their learning models. In 2020 and 2021, educational institutions faced unprecedented challenges, transitioning between in-person, hybrid, and virtual learning models in response to evolving public health guidelines and infection rates. This shift has brought into sharp focus the need to understand the implications of these various schooling models on educational outcomes.

This paper delves into a comprehensive analysis across 11 U.S. states, examining the relationship between different schooling models and their impact on student enrollment and staffing patterns. The unique dataset compiled for this study encompasses a variety of metrics, including total enrollment, in-person and virtual attendance, as well as staff count across different learning models. By analyzing data from kindergarten to 12th grade, this study provides a granular view of how the pandemic has reshaped the educational landscape across these states.

Our findings reveal significant variations in how states have navigated the challenges posed by the pandemic, with notable differences in the adoption of in-person, hybrid, and virtual learning models. We observe that these variations are not only reflective of public health policies but also indicate broader socio-economic and demographic influences. For instance, preliminary analysis suggests that shifts to virtual learning correlate with changes in student enrollment, raising questions about equity and access in education during these challenging times.

The remainder of the paper is structured as follows: Section 2 provides a detailed overview of the data and methodology employed in this study. Section 3 presents an in-depth analysis of the schooling models across the 11 states, offering insights into the trends and patterns observed in the data. Section 4 discusses the key findings, exploring the implications of these schooling models on educational outcomes. Finally, Section 5 concludes with a reflection on the study's findings, limitations, and potential directions for future research in this critical area of educational policy.

The remainder of this paper is structured as follows. @sec-data....



# Data {#sec-data}

Some of our data is of penguins (@fig-bills), from @palmerpenguins.

```{r}
#| label: fig-bills
#| fig-cap: Bills of penguins
#| echo: false

library(dplyr)

# Assuming each state data frame has the same structure and columns
all_states <- bind_rows(
  colorado %>% mutate(state = "Colprado"),
  connecticut %>% mutate(state = "Connecticut"),
  massachusets %>% mutate(state = "Massachusets"),
  minnesota %>% mutate(state = "Minnesota"),
  mississippi %>% mutate(state = "Mississsippi"),
  ohio %>% mutate(state = "Ohio"),
  rhode %>% mutate(state = "Rhode Islands"),
  virginia %>% mutate(state = "Virginia"),
  westvirginia %>% mutate(state = "West Virginia"),
  winscosin %>% mutate(state = "Winscosin"),
  wyoming %>% mutate(state = "Wyoming"),
  
)

all_states$time_period_start <- as.Date(all_states$time_period_start, format = "%Y-%m-%d")
all_states$learning_model <- as.factor(all_states$learning_model)

library(ggplot2)

# ggplot(all_states, aes(x = time_period_start, y = learning_model, color = state)) +
#   geom_line() +
#   labs(title = "Comparison of Learning Models in US States Over Time",
#        x = "Time Period Start", y = "Learning Model") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

# ggplot(all_states, aes(x = time_period_start, y = learning_model, group = state, color = state)) +
#   geom_line() +
#   scale_x_date(date_breaks = "1 month", date_labels = "%b %Y") +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Rotate x-axis labels if needed
# 

#tthis graph is correct, but causing scale bias, since data is large for some states like Minnesota has 21000 rows, but low for few states like Colorado which only has 1600 rows,
# learning_model_summary <- all_states %>%
#   count(state, learning_model) %>%
#   ungroup()
# 
# # Plotting with a bar graph
# ggplot(learning_model_summary, aes(x = state, y = n, fill = learning_model)) + 
#   geom_bar(stat = "identity", position = "dodge") +
#   theme_minimal() +
#   labs(x = "State", y = "Frequency of Learning Model", fill = "Learning Model", 
#        title = "Comparison of Learning Models in US States") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))


#therefore we scale it first, by taking proportions:
state_totals <- all_states %>%
  group_by(state) %>%
  summarise(total = n())

# Calculate the proportion of each learning model within each state
all_states <- all_states %>%
  group_by(state, learning_model) %>%
  summarise(count = n(), .groups = 'drop') %>%
  left_join(state_totals, by = "state") %>%
  mutate(proportion = count / total)

# Plotting with a bar graph
# ggplot(all_states, aes(x = state, y = proportion, fill = learning_model)) + 
#   geom_bar(stat = "identity", position = "dodge") +
#   theme_minimal() +
#   labs(x = "State", y = "Proportion", fill = "Learning Model", 
#        title = "Proportional Comparison of Learning Models in US States") +
  # scale_y_continuous(labels = scales::percent)

ggplot(all_states, aes(x = state, y = proportion, fill = learning_model)) + 
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  theme_minimal() +
  labs(x = "State", y = "Proportion of Leaning Models", fill = "Learning Model", 
       title = "Proportional Comparison of Learning Models in US States") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
        plot.title = element_text(hjust = 0.5))

```

Talk more about it.

And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)

```{r}
#| label: fig-planes
#| fig-cap: Relationship between wing length and width
#| echo: false
#| warning: false
#| message: false


```

Talk way more about it. 



# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.







# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 



\newpage


# References


